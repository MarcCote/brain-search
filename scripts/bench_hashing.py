#!/usr/bin/env python
from __future__ import division

import os
from os.path import join as pjoin

import json
import time
import numpy as np
import pylab as plt
import itertools
import nibabel as nib

from itertools import izip, chain
import brainsearch.vizu as vizu

import brainsearch.utils as brainutil
#from brainsearch.imagespeed import blockify
from brainsearch.brain_database import BrainDatabaseManager
from brainsearch.brain_data import brain_data_factory

from nearpy.hashes import RandomBinaryProjections, RandomPCABinaryProjections, PCABinaryProjections, SpectralHashing
from nearpy.distances import EuclideanDistance
from nearpy.filters import NearestFilter, DistanceThresholdFilter
from nearpy.utils import chunk, ichunk

from brainsearch.brain_processing import BrainPipelineProcessing, BrainNormalization, BrainResampling

import argparse

#PORT = 4242
PORT = 6379
OFFSET = 0.01


def build_subcommand_list(subparser):
    DESCRIPTION = "List available brain databases."

    p = subparser.add_parser("list",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('name', type=str, nargs='?', help='name of the brain database')
    p.add_argument('-v', action='store_true', help='display more information about brain databases')
    p.add_argument('-f', action='store_true', help='check integrity of brain databases')


def build_subcommand_clear(subparser):
    DESCRIPTION = "Clear brain databases."

    p = subparser.add_parser("clear",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('names', metavar="name", type=str, nargs="*", help='name of the brain database to delete')
    p.add_argument('-f', action='store_true', help='clear also metadata')


def build_subcommand_init(subparser):
    DESCRIPTION = "Build a new brain database (nearpy's engine)."

    p = subparser.add_parser("init",
                             description=DESCRIPTION,
                             help=DESCRIPTION)

    p.add_argument('name', type=str, help='name of the brain database')
    p.add_argument('shape', metavar="X,Y,...", type=str, help="data's shape or patch shape")
    p.add_argument('--LSH', metavar="N", type=int, nargs="+", help='numbers of random projections')
    p.add_argument('--LSH_PCA', metavar="N", type=int, nargs="+", help='numbers of random projections in PCA space')
    p.add_argument('--PCA', metavar="K", type=int, nargs="+", help='use K eigenvectors')
    p.add_argument('--SH', metavar="K", type=int, nargs="+", help='length of hash codes generated by Spectral Hashing')
    p.add_argument('--trainset', type=str, help='JSON file use to "train" PCA')
    p.add_argument('--pkl', type=str, help='pickle file containing the PCA information of the data')
    p.add_argument('--bounds', type=str, help='pickle file containing the bounds used by spectral hashing')


def build_subcommand_add(subparser):
    DESCRIPTION = "Add data to an existing brain database."

    p = subparser.add_parser("add",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('name', type=str, help='name of the brain database')
    p.add_argument('config', type=str, help='contained in a JSON file')


def build_subcommand_eval(subparser):
    DESCRIPTION = "Evaluate data given an existing brain database."

    p = subparser.add_parser("eval",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('name', type=str, help='name of the brain database')
    p.add_argument('config', type=str, help='contained in a JSON file')
    p.add_argument('-k', type=int, help='consider at most K nearest-neighbors')


def build_subcommand_map(subparser):
    DESCRIPTION = "Create a color map for a brain given an existing brain database."

    p = subparser.add_parser("map",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('name', type=str, help='name of the brain database')
    p.add_argument('config', type=str, help='contained in a JSON file')
    p.add_argument('-k', type=int, help='consider at most K nearest-neighbors', default=100)
    p.add_argument('--prefix', type=str, help="prefix for the name of the results files", default="")
    p.add_argument('--radius', type=int, help="only look at neighbors within a certain radius")


def build_subcommand_vizu(subparser):
    DESCRIPTION = "Run some vizu for a brain given an existing brain database."

    p = subparser.add_parser("vizu",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('name', type=str, help='name of the brain database')
    p.add_argument('config', type=str, help='contained in a JSON file')


def build_subcommand_check(subparser):
    DESCRIPTION = "Check candidates distribution given an existing brain database."

    p = subparser.add_parser("check",
                             description=DESCRIPTION,
                             help=DESCRIPTION,
                             formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    p.add_argument('name', type=str, help='name of the brain database')
    p.add_argument('config', type=str, nargs='?', help='contained in a JSON file')
    #p.add_argument('-m', dest="min_nonempty", type=int, help='consider only patches having this minimum number of non-empty voxels')


def buildArgsParser():
    DESCRIPTION = "Script to perform brain searches."
    p = argparse.ArgumentParser(description=DESCRIPTION)

    p.add_argument('--storage', type=str, default="redis", help='which storage to use: redis, memory, file')

    p.add_argument('--use_spatial_code', action='store_true', help='include spatial position of a patch in hashcode')
    p.add_argument('-m', dest="min_nonempty", type=int, help='consider only patches having this minimum number of non-empty voxels')
    p.add_argument('-N', type=int, help='use N brains', default=10)
    p.add_argument('-r', dest="resampling_factor", type=float, help='resample image before processing', default=1.)
    p.add_argument('--norm', dest="do_normalization", action="store_true", help='perform histogram equalization')

    subparser = p.add_subparsers(title="brain_search commands", metavar="", dest="command")
    build_subcommand_list(subparser)
    build_subcommand_init(subparser)
    build_subcommand_add(subparser)
    build_subcommand_eval(subparser)
    build_subcommand_map(subparser)
    build_subcommand_vizu(subparser)
    build_subcommand_check(subparser)
    build_subcommand_clear(subparser)

    return p


def main(brain_manager=None):
    parser = buildArgsParser()
    args = parser.parse_args()

    if brain_manager is None:
        brain_manager = BrainDatabaseManager(args.storage)

    # Build processing pipeline
    pipeline = BrainPipelineProcessing()
    if args.do_normalization:
        pipeline.add(BrainNormalization(type=0))
    if args.resampling_factor > 1:
        pipeline.add(BrainResampling(args.resampling_factor))

    if args.command == "list":
        def print_info(name, brain_db):
            print name
            #print "\tMetadata:", brain_db.metadata
            print "\tPatch size:", brain_db.metadata["patch"].shape
            print "\tHashes:", map(str, brain_db.engine.lshashes)
            if args.v:
                labels_counts = ["{}: {:,}".format(i, label_count) for i, label_count in enumerate(brain_db.labels_count(check_integrity=args.f))]
                print "\tLabels: {" + "; ".join(labels_counts) + "}"
                print "\tPatches: {:,}".format(brain_db.nb_patches(check_integrity=args.f))
                print "\tBuckets: {:,}".format(brain_db.nb_buckets())

        if args.name in brain_manager:
            print_info(args.name, brain_manager[args.name])
        else:
            print "Available brain databases: "
            for name, brain_db in brain_manager.brain_databases.items():
                try:
                    print_info(name, brain_db)
                except:
                    import traceback
                    traceback.print_exc()
                    print "*Brain database '{}' is corrupted!*\n".format(name)

    elif args.command == "clear":
        start = time.time()
        if len(args.names) == 0:
            print "Clearing all"
            brain_manager.remove_all_brain_databases(args.f)
        else:
            for name in args.names:
                brain_db = brain_manager[name]
                if brain_db is None:
                    raise ValueError("Unexisting brain database: " + name)

                print "Clearing", name
                brain_manager.remove_brain_database(brain_db, args.f)

        print "Done in {:.2f} sec.".format(time.time()-start)

    elif args.command == "init":
        print "Creating brain database {}...".format(args.name)
        if args.name in brain_manager:
            print ("This database already exists. Please use command "
                   "'brain_search.py --storage {} clear -f {}' before.".format(args.storage, args.name))
            exit()

        start = time.time()
        patch_shape = tuple(map(int, args.shape.split(",")))

        def _get_all_patches():
            config = json.load(open(args.trainset))
            brain_data = brain_data_factory(config, pipeline=pipeline)
            for brain_id, brain in enumerate(brain_data):
                print "ID: {0}/{1}".format(brain_id, len(brain_data))
                if brain_id == args.N:
                    break
                patches, positions = brain.extract_patches(patch_shape, min_nonempty=args.min_nonempty, with_positions=True)
                vectors = patches.reshape((-1, np.prod(patch_shape)))

                if args.use_spatial_code:
                    # Normalize position
                    pos_normalized = positions / np.array(brain.infos['img_shape'], dtype="float32")
                    vectors = np.c_[pos_normalized, vectors]

                yield vectors

        dimension = np.prod(patch_shape)
        if args.use_spatial_code:
            dimension += len(patch_shape)

        hashes = []
        if args.SH is not None:
            for nbits in args.SH:
                hash_name = "SH{nbits}".format(nbits=nbits)
                hashes.append(SpectralHashing(hash_name, nbits=nbits, dimension=dimension,
                                              trainset=_get_all_patches,
                                              pkl=args.pkl, bounds=args.bounds))

        if args.LSH_PCA is not None:
            for nb_projections in args.LSH_PCA:
                hash_name = "LSH_PCA{nb_projections}".format(nb_projections=nb_projections)
                hashes.append(RandomPCABinaryProjections(hash_name, projection_count=nb_projections, dimension=np.prod(patch_shape), trainset=_get_all_patches(), pkl=args.pkl))

        if args.PCA is not None:
            for nbits in args.PCA:
                hash_name = "PCA{nbits}".format(nbits=nbits)
                hashes.append(PCABinaryProjections(hash_name, dimension=np.prod(patch_shape),
                              trainset=_get_all_patches, nbits=nbits, pkl=args.pkl))

        if args.LSH is not None:
            for nb_projections in args.LSH:
                    hash_name = "LSH{nb_projections}".format(nb_projections=nb_projections)
                    hashes.append(RandomBinaryProjections(hash_name, nb_projections, dimension=np.prod(patch_shape)))

        metadata = {b"patch": {"dtype": np.dtype(np.float32).str, "shape": patch_shape},
                    b"label": {"dtype": np.dtype(np.int8).str, "shape": (1,)},
                    b"id": {"dtype": np.dtype(np.int32).str, "shape": (1,)},
                    b"position": {"dtype": np.dtype(np.int32).str, "shape": (len(patch_shape),)},
                    }
        brain_manager.new_brain_database(args.name, hashes[0], metadata)
        print "Created in {0:.2f} sec.".format(time.time()-start)

    elif args.command == "add":
        brain_db = brain_manager[args.name]
        if brain_db is None:
            raise ValueError("Unexisting brain database: " + args.name)

        patch_shape = tuple(brain_db.metadata['patch'].shape)
        config = json.load(open(args.config))
        brain_data = brain_data_factory(config, pipeline=pipeline)

        print 'Inserting...'
        nb_elements_total = 0
        start = time.time()
        for brain_id, brain in enumerate(brain_data):
            if brain_id == args.N:
                break

            start_brain = time.time()
            #image = brain.process(args.resampling_factor, args.do_normalization)
            #image, affine = brain.resample(args.resampling_factor)
            #patches, datainfo = get_patches_with_info(image, brain_id, brain.label, patch_shape=patch_shape, min_nonempty=args.min_nonempty)

            start_extracting = time.time()
            patches, datainfo = brain.extract_patches(patch_shape, min_nonempty=args.min_nonempty, with_info=True)
            print "extracting: {:.2f}".format(time.time()-start_extracting)

            vectors = patches.reshape((-1, np.prod(patch_shape)))
            if args.use_spatial_code:
                # Normalize position
                pos_normalized = datainfo["position"] / np.array(brain.infos['img_shape'], dtype="float32")
                pos_normalized = pos_normalized.astype("float32")
                vectors = np.c_[pos_normalized, vectors]

            hashkeys = brain_db.insert(vectors, datainfo["patch"], datainfo["label"], datainfo["position"], datainfo["id"])

            print "ID: {0} (label:{3}), {1:,} patches in {2:.2f} sec.".format(brain_id, len(hashkeys), time.time()-start_brain, brain.label)
            nb_elements_total += len(hashkeys)

        print "Inserted {0:,} patches ({1} brains) in {2:.2f} sec.".format(nb_elements_total, brain_id+1, time.time()-start)

    elif args.command == "check":
        brain_db = brain_manager[args.name]
        if brain_db is None:
            raise ValueError("Unexisting brain database: " + args.name)

        if args.config is None:
            # Simply report stats about buckets size.
            print 'Counting...'
            start = time.time()
            sizes, bucketkeys = brain_db.buckets_size()
            print "Counted {2:,} candidates for {0:,} buckets in {1:.2f} sec.".format(len(sizes), time.time()-start, sum(sizes))
            print "Avg. candidates per bucket: {0:.2f}".format(np.mean(sizes))
            print "Std. candidates per bucket: {0:.2f}".format(np.std(sizes))
            print "Min. candidates per bucket: {0:,}".format(np.min(sizes))
            print "Max. candidates per bucket: {0:,}".format(np.max(sizes))

    # elif args.command == "eval":
    #     brain_database = brain_manager[args.name]
    #     if brain_database is None:
    #         raise ValueError("Unexisting brain database: " + args.name)

    #     patch_shape = tuple(brain_database.config['shape'])
    #     config = json.load(open(args.config))
    #     brain_data = brain_data_factory(config)

    #     print 'Evaluating...'

    #     def majority_vote(candidates):
    #         return np.argmax(np.mean([np.array(c['data']['target']) for c in candidates], axis=0))

    #     def weighted_majority_vote(candidates):
    #         votes = [np.exp(-c['dist']) * np.array(c['data']['target']) for c in candidates]
    #         return np.argmax(np.mean(votes, axis=0))

    #     brain_database.engine.distance = EuclideanDistance()

    #     #neighbors = []
    #     nb_neighbors = 0
    #     start = time.time()
    #     nb_success = 0.0
    #     nb_patches = 0
    #     for brain_id, (brain, label) in enumerate(brain_data):
    #         patches_and_pos = get_patches(brain, patch_shape=patch_shape, min_nonempty=args.min_nonempty)
    #         patches = flattenize((patch for patch, pos in patches_and_pos))

    #         start_brain = time.time()
    #         neighbors_per_patch = brain_database.query(patches)
    #         nb_patches += len(neighbors_per_patch)
    #         brain_neighbors = list(chain(*neighbors_per_patch))
    #         print "Brain #{0} ({3:,} patches), found {1:,} neighbors in {2:.2f} sec.".format(brain_id, len(brain_neighbors), time.time()-start_brain, len(neighbors_per_patch))

    #         #prediction = weighted_majority_vote(brain_neighbors)
    #         prediction = majority_vote(brain_neighbors)
    #         nb_success += prediction == np.argmax(label)

    #         nb_neighbors += len(brain_neighbors)
    #         del brain_neighbors
    #         #neighbors.extend(brain_neighbors)

    #     nb_brains = brain_id + 1
    #     print "Found a total of {0:,} neighbors for {1} brains ({3:,} patches) in {2:.2f} sec.".format(nb_neighbors, nb_brains, time.time()-start, nb_patches)
    #     print "Classification error: {:2.2f}%".format(100 * (1. - nb_success/nb_brains))

    elif args.command == "vizu":
        from brainsearch.vizu_chaco import NoisyBrainsearchViewer

        brain_db = brain_manager[args.name]
        if brain_db is None:
            raise ValueError("Unexisting brain database: " + args.name)

        patch_shape = brain_db.metadata['patch'].shape
        config = json.load(open(args.config))
        brain_data = brain_data_factory(config, pipeline=pipeline)

        for brain_id, brain in enumerate(brain_data):
            print 'Viewing brain #{0} (label: {1})'.format(brain_id, brain.label)
            patches, positions = brain.extract_patches(patch_shape, min_nonempty=args.min_nonempty, with_positions=True)

            query = {'patches': patches,
                     'positions': positions,
                     'patch_size': patch_shape}
            viewer = NoisyBrainsearchViewer(query, brain_db.engine, brain_voxels=brain.image)
            viewer.configure_traits()

    return brain_manager


if __name__ == '__main__':
    db_manager = main()
    # import sys
    # sys.argv = "brain_search.py --storage memory list".split()
    # db_manager = main(db_manager)

    # sys.argv = "brain_search.py --storage memory -m 3 add SPECTRAL data/FA/PPMI_trainset.json".split()
    # db_manager = main(db_manager)
